{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Funcparserlib Recursive descent parsing library for Python based on functional combinators. Description The primary focus of funcparserlib is parsing little languages or external DSLs (domain specific languages). Parsers made with funcparserlib are pure-Python LL(*) parsers. It means that it's very easy to write parsers without thinking about lookaheads and other hardcore parsing stuff. However, recursive descent parsing is a rather slow method compared to LL(k) or LR(k) algorithms. Still, parsing with funcparserlib is at least twice faster than PyParsing , a very popular library for Python. The source code of funcparserlib is only 1.2K lines of code, with lots of comments. Its API is fully type hinted. It features the longest parsed prefix error reporting, as well as a tiny lexer generator for token position tracking. The idea of parser combinators used in funcparserlib comes from the Introduction to Functional Programming course. We have converted it from ML into Python. Installation You can install funcparserlib from PyPI : $ pip install funcparserlib There are no dependencies on other libraries. Documentation Getting Started Your starting point with funcparserlib API Reference Learn the details of the API There are several examples available in the tests/ directory: GraphViz DOT parser JSON parser See also the changelog . Example Let's consider a little language of numeric expressions with a syntax similar to Python expressions. Here are some expression strings in this language: 0 1 + 2 + 3 -1 + 2 ** 32 3.1415926 * (2 + 7.18281828e-1) * 42 Here is the complete source code of the tokenizer and the parser for this language written using funcparserlib : from typing import List , Tuple , Union from dataclasses import dataclass from funcparserlib.lexer import make_tokenizer , TokenSpec , Token from funcparserlib.parser import tok , Parser , many , forward_decl , finished @dataclass class BinaryExpr : op : str left : \"Expr\" right : \"Expr\" Expr = Union [ BinaryExpr , int , float ] def tokenize ( s : str ) -> List [ Token ]: specs = [ TokenSpec ( \"whitespace\" , r \"\\s+\" ), TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ] tokenizer = make_tokenizer ( specs ) return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] def parse ( tokens : List [ Token ]) -> Expr : int_num = tok ( \"int\" ) >> int float_num = tok ( \"float\" ) >> float number = int_num | float_num expr : Parser [ Token , Expr ] = forward_decl () parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) primary = number | parenthesized power = primary + many ( op ( \"**\" ) + primary ) >> to_expr term = power + many (( op ( \"*\" ) | op ( \"/\" )) + power ) >> to_expr sum = term + many (( op ( \"+\" ) | op ( \"-\" )) + term ) >> to_expr expr . define ( sum ) document = expr + - finished return document . parse ( tokens ) def op ( name : str ) -> Parser [ Token , str ]: return tok ( \"op\" , name ) def to_expr ( args : Tuple [ Expr , List [ Tuple [ str , Expr ]]]) -> Expr : first , rest = args result = first for op , expr in rest : result = BinaryExpr ( op , result , expr ) return result Now, consider this numeric expression: 3.1415926 * (2 + 7.18281828e-1) * 42 . Let's tokenize() it using the tokenizer we've created with funcparserlib.lexer : [ Token('float', '3.1415926'), Token('op', '*'), Token('op', '('), Token('int', '2'), Token('op', '+'), Token('float', '7.18281828e-1'), Token('op', ')'), Token('op', '*'), Token('int', '42'), ] Let's parse() these tokens into an expression tree using our parser created with funcparserlib.parser : BinaryExpr( op='*', left=BinaryExpr( op='*', left=3.1415926, right=BinaryExpr(op='+', left=2, right=0.718281828), ), right=42, ) Learn how to write this parser using funcparserlib in the Getting Started guide! Used By Some open-source projects that use funcparserlib as an explicit dependency: Hy , a Lisp dialect that's embedded in Python 4.2K stars, version >= 1.0.0a0 , Python 3.7+ Spash , a JavaScript rendering service with HTTP API, by Scrapinghub 3.6K stars, version * . Python 3 in Docker graphite-beacon , a simple alerting system for Graphite metrics 459 stars, version ==0.3.6 , Python 2 and 3 blockdiag , generates block-diagram image file from spec-text file 148 stars, version >= 1.0.0a0 , Python 3.7+ kll , Keyboard Layout Language (KLL) compiler 109 stars, copied source code, Python 3.5+ Next Read the Getting Started guide to start learning funcparserlib .","title":"Home"},{"location":"#funcparserlib","text":"Recursive descent parsing library for Python based on functional combinators.","title":"Funcparserlib"},{"location":"#description","text":"The primary focus of funcparserlib is parsing little languages or external DSLs (domain specific languages). Parsers made with funcparserlib are pure-Python LL(*) parsers. It means that it's very easy to write parsers without thinking about lookaheads and other hardcore parsing stuff. However, recursive descent parsing is a rather slow method compared to LL(k) or LR(k) algorithms. Still, parsing with funcparserlib is at least twice faster than PyParsing , a very popular library for Python. The source code of funcparserlib is only 1.2K lines of code, with lots of comments. Its API is fully type hinted. It features the longest parsed prefix error reporting, as well as a tiny lexer generator for token position tracking. The idea of parser combinators used in funcparserlib comes from the Introduction to Functional Programming course. We have converted it from ML into Python.","title":"Description"},{"location":"#installation","text":"You can install funcparserlib from PyPI : $ pip install funcparserlib There are no dependencies on other libraries.","title":"Installation"},{"location":"#documentation","text":"Getting Started Your starting point with funcparserlib API Reference Learn the details of the API There are several examples available in the tests/ directory: GraphViz DOT parser JSON parser See also the changelog .","title":"Documentation"},{"location":"#example","text":"Let's consider a little language of numeric expressions with a syntax similar to Python expressions. Here are some expression strings in this language: 0 1 + 2 + 3 -1 + 2 ** 32 3.1415926 * (2 + 7.18281828e-1) * 42 Here is the complete source code of the tokenizer and the parser for this language written using funcparserlib : from typing import List , Tuple , Union from dataclasses import dataclass from funcparserlib.lexer import make_tokenizer , TokenSpec , Token from funcparserlib.parser import tok , Parser , many , forward_decl , finished @dataclass class BinaryExpr : op : str left : \"Expr\" right : \"Expr\" Expr = Union [ BinaryExpr , int , float ] def tokenize ( s : str ) -> List [ Token ]: specs = [ TokenSpec ( \"whitespace\" , r \"\\s+\" ), TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ] tokenizer = make_tokenizer ( specs ) return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] def parse ( tokens : List [ Token ]) -> Expr : int_num = tok ( \"int\" ) >> int float_num = tok ( \"float\" ) >> float number = int_num | float_num expr : Parser [ Token , Expr ] = forward_decl () parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) primary = number | parenthesized power = primary + many ( op ( \"**\" ) + primary ) >> to_expr term = power + many (( op ( \"*\" ) | op ( \"/\" )) + power ) >> to_expr sum = term + many (( op ( \"+\" ) | op ( \"-\" )) + term ) >> to_expr expr . define ( sum ) document = expr + - finished return document . parse ( tokens ) def op ( name : str ) -> Parser [ Token , str ]: return tok ( \"op\" , name ) def to_expr ( args : Tuple [ Expr , List [ Tuple [ str , Expr ]]]) -> Expr : first , rest = args result = first for op , expr in rest : result = BinaryExpr ( op , result , expr ) return result Now, consider this numeric expression: 3.1415926 * (2 + 7.18281828e-1) * 42 . Let's tokenize() it using the tokenizer we've created with funcparserlib.lexer : [ Token('float', '3.1415926'), Token('op', '*'), Token('op', '('), Token('int', '2'), Token('op', '+'), Token('float', '7.18281828e-1'), Token('op', ')'), Token('op', '*'), Token('int', '42'), ] Let's parse() these tokens into an expression tree using our parser created with funcparserlib.parser : BinaryExpr( op='*', left=BinaryExpr( op='*', left=3.1415926, right=BinaryExpr(op='+', left=2, right=0.718281828), ), right=42, ) Learn how to write this parser using funcparserlib in the Getting Started guide!","title":"Example"},{"location":"#used-by","text":"Some open-source projects that use funcparserlib as an explicit dependency: Hy , a Lisp dialect that's embedded in Python 4.2K stars, version >= 1.0.0a0 , Python 3.7+ Spash , a JavaScript rendering service with HTTP API, by Scrapinghub 3.6K stars, version * . Python 3 in Docker graphite-beacon , a simple alerting system for Graphite metrics 459 stars, version ==0.3.6 , Python 2 and 3 blockdiag , generates block-diagram image file from spec-text file 148 stars, version >= 1.0.0a0 , Python 3.7+ kll , Keyboard Layout Language (KLL) compiler 109 stars, copied source code, Python 3.5+","title":"Used By"},{"location":"#next","text":"Read the Getting Started guide to start learning funcparserlib .","title":"Next"},{"location":"changes/","text":"The Changelog 1.0.0 (to be released) The stable 1.0.0 release freezes the API of funcparserlib 0.3.6 which was released on 2013-05-02. Added Added support for Python 3.10 Added support for Python 3.9 ( #63 ) (Thanks to @pkulev ) Added support for Python 3.8 Added -p (the same as skip(p) ) with more strict type hints for -p and p1 + p2 Added tok(type[, value]) for more compact grammars, better error messages Added TokenSpec(type, pattern[, flags]) to simplify the use of make_tokenizer() Added type hints for the public API Added the new library homepage with the new Getting Started guide and the new API reference Changed Parse exceptions now show expected tokens and grammar rules at the stopped position ( #52 ) Dropped support for Python 3.4, 3.5, 3.6 (end of life) Dropped support for Python 2.5, 2.6, 3.3 (end of life), modernized code for Python 3 to run without obsolete 2to3 ( #57 ) (Thanks to @jdufresne ) Removed documentation and unit tests from the distribution Switched from setuptools to Poetry Switched to poetry-core for lighter PEP 517 builds ( #73 ) (Thanks to @fabaff ) Run unit tests on GitHub Actions for all supported Pythons Fixed Fixed TypeError in oneplus when applying it parser + parser ( #66 ) (Thanks to @martica ) Fixed AttributeError when comparing Token objects to None ( #58 ) (Thanks to @Halolegend94 ) Fixed doctests in the tutorial ( #49 ) Fixed several cases of wrong expected tokens in error messages 0.3.6 \u2014 2013-05-02 Changed Python 3 compatibility More info available in exception objects ( #14 ) Fixed Fixed many() that consumed too many tokens in some cases ( #31 ) 0.3.5 \u2014 2011-01-13 Changed Python 2.4 compatibility More readable terminal names for error reporting Fixed Fixed wrong token positions in lexer error messages 0.3.4 \u2014 2009-10-06 Changed Switched from setuptools to distutils Improved the run-tests utility Fixed Fixed importing all symbols from funcparserlib.lexer 0.3.3 \u2014 2009-08-03 Added Added a FAQ question about infinite loops in parsers Changed Debug rule tracing can be enabled again Fixed Fixed a bug in results of skip + skip parsers 0.3.2 \u2014 2009-07-26 Added Added the Parsing Stages Illustrated page Fixed Fixed some string and number encoding issues in examples 0.3.1 \u2014 2009-07-26 Major optimizations (10x faster than the version 0.3). Added Added the forward_decl function, that performs better than with_forward_decls Added the pretty_tree function for creating pseudo-graphic trees Added the Nested Brackets Mini-HOWTO Added Makefile and this CHANGES.md file Changed Use a single immutable input sequence in parsers Call a wrapped parser directly using run (without __call__ ) The slow logging is enabled only when the debug flag is set 0.3 \u2014 2009-07-23 Added Added pure and bind functions on Parser s making them monads Added the Funcparserlib Tutorial Added a JSON parser as an example Changed Translated the docs from Russian into English 0.2 \u2014 2009-07-07 Added Added the with_forward_decls combinator for dealing with forward declarations Changed Switched to the iterative implementation of many Un-curried the parser function type in order to simplify things Improvements to the DOT parser 0.1 \u2014 2009-06-26 Initial release.","title":"Changelog"},{"location":"changes/#the-changelog","text":"","title":"The Changelog"},{"location":"changes/#100-to-be-released","text":"The stable 1.0.0 release freezes the API of funcparserlib 0.3.6 which was released on 2013-05-02.","title":"1.0.0 (to be released)"},{"location":"changes/#added","text":"Added support for Python 3.10 Added support for Python 3.9 ( #63 ) (Thanks to @pkulev ) Added support for Python 3.8 Added -p (the same as skip(p) ) with more strict type hints for -p and p1 + p2 Added tok(type[, value]) for more compact grammars, better error messages Added TokenSpec(type, pattern[, flags]) to simplify the use of make_tokenizer() Added type hints for the public API Added the new library homepage with the new Getting Started guide and the new API reference","title":"Added"},{"location":"changes/#changed","text":"Parse exceptions now show expected tokens and grammar rules at the stopped position ( #52 ) Dropped support for Python 3.4, 3.5, 3.6 (end of life) Dropped support for Python 2.5, 2.6, 3.3 (end of life), modernized code for Python 3 to run without obsolete 2to3 ( #57 ) (Thanks to @jdufresne ) Removed documentation and unit tests from the distribution Switched from setuptools to Poetry Switched to poetry-core for lighter PEP 517 builds ( #73 ) (Thanks to @fabaff ) Run unit tests on GitHub Actions for all supported Pythons","title":"Changed"},{"location":"changes/#fixed","text":"Fixed TypeError in oneplus when applying it parser + parser ( #66 ) (Thanks to @martica ) Fixed AttributeError when comparing Token objects to None ( #58 ) (Thanks to @Halolegend94 ) Fixed doctests in the tutorial ( #49 ) Fixed several cases of wrong expected tokens in error messages","title":"Fixed"},{"location":"changes/#036-2013-05-02","text":"","title":"0.3.6 \u2014 2013-05-02"},{"location":"changes/#changed_1","text":"Python 3 compatibility More info available in exception objects ( #14 )","title":"Changed"},{"location":"changes/#fixed_1","text":"Fixed many() that consumed too many tokens in some cases ( #31 )","title":"Fixed"},{"location":"changes/#035-2011-01-13","text":"","title":"0.3.5 \u2014 2011-01-13"},{"location":"changes/#changed_2","text":"Python 2.4 compatibility More readable terminal names for error reporting","title":"Changed"},{"location":"changes/#fixed_2","text":"Fixed wrong token positions in lexer error messages","title":"Fixed"},{"location":"changes/#034-2009-10-06","text":"","title":"0.3.4 \u2014 2009-10-06"},{"location":"changes/#changed_3","text":"Switched from setuptools to distutils Improved the run-tests utility","title":"Changed"},{"location":"changes/#fixed_3","text":"Fixed importing all symbols from funcparserlib.lexer","title":"Fixed"},{"location":"changes/#033-2009-08-03","text":"","title":"0.3.3 \u2014 2009-08-03"},{"location":"changes/#added_1","text":"Added a FAQ question about infinite loops in parsers","title":"Added"},{"location":"changes/#changed_4","text":"Debug rule tracing can be enabled again","title":"Changed"},{"location":"changes/#fixed_4","text":"Fixed a bug in results of skip + skip parsers","title":"Fixed"},{"location":"changes/#032-2009-07-26","text":"","title":"0.3.2 \u2014 2009-07-26"},{"location":"changes/#added_2","text":"Added the Parsing Stages Illustrated page","title":"Added"},{"location":"changes/#fixed_5","text":"Fixed some string and number encoding issues in examples","title":"Fixed"},{"location":"changes/#031-2009-07-26","text":"Major optimizations (10x faster than the version 0.3).","title":"0.3.1 \u2014 2009-07-26"},{"location":"changes/#added_3","text":"Added the forward_decl function, that performs better than with_forward_decls Added the pretty_tree function for creating pseudo-graphic trees Added the Nested Brackets Mini-HOWTO Added Makefile and this CHANGES.md file","title":"Added"},{"location":"changes/#changed_5","text":"Use a single immutable input sequence in parsers Call a wrapped parser directly using run (without __call__ ) The slow logging is enabled only when the debug flag is set","title":"Changed"},{"location":"changes/#03-2009-07-23","text":"","title":"0.3 \u2014 2009-07-23"},{"location":"changes/#added_4","text":"Added pure and bind functions on Parser s making them monads Added the Funcparserlib Tutorial Added a JSON parser as an example","title":"Added"},{"location":"changes/#changed_6","text":"Translated the docs from Russian into English","title":"Changed"},{"location":"changes/#02-2009-07-07","text":"","title":"0.2 \u2014 2009-07-07"},{"location":"changes/#added_5","text":"Added the with_forward_decls combinator for dealing with forward declarations","title":"Added"},{"location":"changes/#changed_7","text":"Switched to the iterative implementation of many Un-curried the parser function type in order to simplify things Improvements to the DOT parser","title":"Changed"},{"location":"changes/#01-2009-06-26","text":"Initial release.","title":"0.1 \u2014 2009-06-26"},{"location":"api/","text":"API Reference Funcparserlib consists of the following modules: funcparserlib.lexer \u2014 Regexp-based tokenizer funcparserlib.parser \u2014 Functional parsing combinators funcparserlib.util \u2014 Various utilities","title":"API Overview"},{"location":"api/#api-reference","text":"Funcparserlib consists of the following modules: funcparserlib.lexer \u2014 Regexp-based tokenizer funcparserlib.parser \u2014 Functional parsing combinators funcparserlib.util \u2014 Various utilities","title":"API Reference"},{"location":"api/lexer/","text":"funcparserlib.lexer \u2014 Regexp-based tokenizer funcparserlib . lexer . make_tokenizer ( specs ) Make a function that tokenizes text based on the regexp specs. Type: (Sequence[TokenSpec | Tuple]) -> Callable[[str], Iterable[Token]] A token spec is TokenSpec instance. Note For legacy reasons, a token spec may also be a tuple of ( type , args ), where type sets the value of Token.type for the token, and args are the positional arguments for re.compile() : either just ( pattern ,) or ( pattern , flags ). It returns a tokenizer function that takes a string and returns an iterable of Token objects, or raises LexerError if it cannot tokenize the string according to its token specs. Examples: >>> tokenize = make_tokenizer ([ ... TokenSpec ( \"space\" , r \"\\s+\" ), ... TokenSpec ( \"id\" , r \"\\w+\" ), ... TokenSpec ( \"op\" , r \"[,!]\" ), ... ]) >>> text = \"Hello, World!\" >>> [ t for t in tokenize ( text ) if t . type != \"space\" ] # noqa [Token('id', 'Hello'), Token('op', ','), Token('id', 'World'), Token('op', '!')] >>> text = \"Bye?\" >>> list ( tokenize ( text )) Traceback (most recent call last): ... lexer.LexerError : cannot tokenize data: 1,4: \"Bye?\" funcparserlib.lexer.TokenSpec A token specification for generating a lexer via make_tokenizer() . funcparserlib . lexer . TokenSpec . __init__ ( self , type , pattern , flags = 0 ) special Initialize a TokenSpec object. Parameters: Name Type Description Default type str User-defined type of the token (e.g. \"name\" , \"number\" , \"operator\" ) required pattern str Regexp for matching this token type required flags int Regexp flags, the second argument of re.compile() 0 funcparserlib.lexer.Token A token object that represents a substring of certain type in your text. You can compare tokens for equality using the == operator. Tokens also define custom repr() and str() . Attributes: Name Type Description type str User-defined type of the token (e.g. \"name\" , \"number\" , \"operator\" ) value str Text value of the token start Optional[Tuple[int, int]] Start position ( line , column ) end Optional[Tuple[int, int]] End position ( line , column )","title":"Lexer"},{"location":"api/lexer/#funcparserliblexer-regexp-based-tokenizer","text":"","title":"funcparserlib.lexer \u2014 Regexp-based tokenizer"},{"location":"api/lexer/#funcparserlib.lexer.make_tokenizer","text":"Make a function that tokenizes text based on the regexp specs. Type: (Sequence[TokenSpec | Tuple]) -> Callable[[str], Iterable[Token]] A token spec is TokenSpec instance. Note For legacy reasons, a token spec may also be a tuple of ( type , args ), where type sets the value of Token.type for the token, and args are the positional arguments for re.compile() : either just ( pattern ,) or ( pattern , flags ). It returns a tokenizer function that takes a string and returns an iterable of Token objects, or raises LexerError if it cannot tokenize the string according to its token specs. Examples: >>> tokenize = make_tokenizer ([ ... TokenSpec ( \"space\" , r \"\\s+\" ), ... TokenSpec ( \"id\" , r \"\\w+\" ), ... TokenSpec ( \"op\" , r \"[,!]\" ), ... ]) >>> text = \"Hello, World!\" >>> [ t for t in tokenize ( text ) if t . type != \"space\" ] # noqa [Token('id', 'Hello'), Token('op', ','), Token('id', 'World'), Token('op', '!')] >>> text = \"Bye?\" >>> list ( tokenize ( text )) Traceback (most recent call last): ... lexer.LexerError : cannot tokenize data: 1,4: \"Bye?\"","title":"make_tokenizer()"},{"location":"api/lexer/#funcparserlib.lexer.TokenSpec","text":"A token specification for generating a lexer via make_tokenizer() .","title":"TokenSpec"},{"location":"api/lexer/#funcparserlib.lexer.TokenSpec.__init__","text":"Initialize a TokenSpec object. Parameters: Name Type Description Default type str User-defined type of the token (e.g. \"name\" , \"number\" , \"operator\" ) required pattern str Regexp for matching this token type required flags int Regexp flags, the second argument of re.compile() 0","title":"__init__()"},{"location":"api/lexer/#funcparserlib.lexer.Token","text":"A token object that represents a substring of certain type in your text. You can compare tokens for equality using the == operator. Tokens also define custom repr() and str() . Attributes: Name Type Description type str User-defined type of the token (e.g. \"name\" , \"number\" , \"operator\" ) value str Text value of the token start Optional[Tuple[int, int]] Start position ( line , column ) end Optional[Tuple[int, int]] End position ( line , column )","title":"Token"},{"location":"api/parser/","text":"funcparserlib.parser \u2014 Functional parsing combinators Functional parsing combinators. Parsing combinators define an internal domain-specific language (DSL) for describing the parsing rules of a grammar. The DSL allows you to start with a few primitive parsers, then combine your parsers to get more complex ones, and finally cover the whole grammar you want to parse. The structure of the language: Class Parser All the primitives and combinators of the language return Parser objects It defines the main Parser.parse(tokens) method Primitive parsers tok(type, value) , a(value) , some(pred) , forward_decl() , finished Parser combinators p1 + p2 , p1 | p2 , p >> f , -p , maybe(p) , many(p) , oneplus(p) , skip(p) Abstraction Use regular Python variables p = ... # Expression of type Parser to define new rules (non-terminals) of your grammar Every time you apply one of the combinators, you get a new Parser object. In other words, the set of Parser objects is closed under the means of combination. Note We took the parsing combinators language from the book Introduction to Functional Programming and translated it from ML into Python. funcparserlib.parser.Parser A parser object that can parse a sequence of tokens or can be combined with other parsers using + , | , >> , many() , and other parsing combinators. Type: Parser[A, B] The generic variables in the type are: A \u2014 the type of the tokens in the sequence to parse, B \u2014 the type of the parsed value. In order to define a parser for your grammar: You start with primitive parsers by calling a(value) , some(pred) , forward_decl() , finished You use parsing combinators p1 + p2 , p1 | p2 , p >> f , many(p) , and others to combine parsers into a more complex parser You can assign complex parsers to variables to define names that correspond to the rules of your grammar Note The constructor Parser.__init__() is considered internal and may be changed in future versions. Use primitive parsers and parsing combinators to construct new parsers. funcparserlib . parser . Parser . parse ( self , tokens ) Parse the sequence of tokens and return the parsed value. Type: (Sequence[A]) -> B It takes a sequence of tokens of arbitrary type A and returns the parsed value of arbitrary type B . If the parser fails to parse the tokens, it raises NoParseError . Note Although Parser.parse() can parse sequences of any objects (including str which is a sequence of str chars), the recommended way is parsing sequences of Token objects. You should use a regexp-based tokenizer make_tokenizer() defined in funcparserlib.lexer to convert your text into a sequence of Token objects before parsing it. You will get more readable parsing error messages (as Token objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar. funcparserlib . parser . Parser . define ( self , p ) Define the parser created earlier as a forward declaration. Type: (Parser[A, B]) -> None Use p = forward_decl() in combination with p.define(...) to define recursive parsers. See the examples in the docs for forward_decl() . funcparserlib . parser . Parser . named ( self , name ) Specify the name of the parser for easier debugging. Type: (str) -> Parser[A, B] This name is used in the debug-level parsing log. You can also get it via the Parser.name attribute. Examples: >>> expr = ( a ( \"x\" ) + a ( \"y\" )) . named ( \"expr\" ) >>> expr . name 'expr' >>> expr = a ( \"x\" ) + a ( \"y\" ) >>> expr . name \"('x', 'y')\" Note You can enable the parsing log this way: import logging logging . basicConfig ( level = logging . DEBUG ) import funcparserlib.parser funcparserlib . parser . debug = True The way to enable the parsing log may be changed in future versions. Primitive Parsers funcparserlib . parser . tok ( type , value = None ) Return a parser that parses a Token and returns the string value of the token. Type: (str, Optional[str]) -> Parser[Token, str] You can match any token of the specified type or you can match a specific token by its type and value . Examples: >>> expr = tok ( \"expr\" ) >>> expr . parse ([ Token ( \"expr\" , \"foo\" )]) 'foo' >>> expr . parse ([ Token ( \"expr\" , \"bar\" )]) 'bar' >>> expr . parse ([ Token ( \"op\" , \"=\" )]) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: '=', expected: expr >>> expr = tok ( \"op\" , \"=\" ) >>> expr . parse ([ Token ( \"op\" , \"=\" )]) '=' >>> expr . parse ([ Token ( \"op\" , \"+\" )]) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: '+', expected: '=' Note In order to convert your text to parse into a sequence of Token objects, use a regexp-based tokenizer make_tokenizer() defined in funcparserlib.lexer . You will get more readable parsing error messages (as Token objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar. funcparserlib . parser . a ( value ) Return a parser that parses a token if it's equal to value . Type: (A) -> Parser[A, A] Examples: >>> expr = a ( \"x\" ) >>> expr . parse ( \"x\" ) 'x' >>> expr . parse ( \"y\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'y', expected: 'x' Note Although Parser.parse() can parse sequences of any objects (including str which is a sequence of str chars), the recommended way is parsing sequences of Token objects. You should use a regexp-based tokenizer make_tokenizer() defined in funcparserlib.lexer to convert your text into a sequence of Token objects before parsing it. You will get more readable parsing error messages (as Token objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar. funcparserlib . parser . some ( pred ) Return a parser that parses a token if it satisfies the predicate pred . Type: (Callable[[A], bool]) -> Parser[A, A] Examples: >>> expr = some ( lambda s : s . isalpha ()) . named ( 'alpha' ) >>> expr . parse ( \"x\" ) 'x' >>> expr . parse ( \"y\" ) 'y' >>> expr . parse ( \"1\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: '1', expected: alpha Warning The some() combinator is quite slow and may be changed or removed in future versions. If you need a parser for a token by its type (e.g. any identifier) and maybe its value, use tok(type[, value]) instead. You should use make_tokenizer() from funcparserlib.lexer to tokenize your text first. funcparserlib . parser . forward_decl () Return an undefined parser that can be used as a forward declaration. Type: Parser[Any, Any] Use p = forward_decl() in combination with p.define(...) to define recursive parsers. Examples: >>> expr = forward_decl () >>> expr . define ( a ( \"x\" ) + maybe ( expr ) + a ( \"y\" )) >>> expr . parse ( \"xxyy\" ) # noqa ('x', ('x', None, 'y'), 'y') >>> expr . parse ( \"xxy\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected end of input, expected: 'y' Note If you care about static types, you should add a type hint for your forward declaration, so that your type checker can check types in p.define(...) later: p : Parser [ str , int ] = forward_decl () p . define ( a ( \"x\" )) # Type checker error p . define ( a ( \"1\" ) >> int ) # OK finished A parser that throws an exception if there are any unparsed tokens left in the sequence. Type: Parser[Any, None] Examples: >>> from funcparserlib.parser import a , finished >>> expr = a ( \"x\" ) + finished >>> expr . parse ( \"x\" ) ('x', None) >>> expr = a ( \"x\" ) + finished >>> expr . parse ( \"xy\" ) Traceback (most recent call last): ... funcparserlib.parser.NoParseError : got unexpected token: 'y', expected: end of input Parser Combinators funcparserlib . parser . Parser . __add__ ( self , other ) special Sequential combination of parsers. It runs this parser, then the other parser. The return value of the resulting parser is a tuple of each parsed value in the sum of parsers. We merge all parsing results of p1 + p2 + ... + pN into a single tuple. It means that the parsing result may be a 2-tuple, a 3-tuple, a 4-tuple, etc. of parsed values. You avoid this by transforming the parsed pair into a new value using the >> combinator. You can also skip some parsing results in the resulting parsers by using -p or skip(p) for some parsers in your sum of parsers. It means that the parsing result might be a single value, not a tuple of parsed values. See the docs for Parser.__neg__() for more examples. Overloaded types (lots of them to provide stricter checking for the quite dynamic return type of this method): (self: Parser[A, B], _IgnoredParser[A]) -> Parser[A, B] (self: Parser[A, B], Parser[A, C]) -> _TupleParser[A, Tuple[B, C]] (self: _TupleParser[A, B], _IgnoredParser[A]) -> _TupleParser[A, B] (self: _TupleParser[A, B], Parser[A, Any]) -> Parser[A, Any] (self: _IgnoredParser[A], _IgnoredParser[A]) -> _IgnoredParser[A] (self: _IgnoredParser[A], Parser[A, C]) -> Parser[A, C] Examples: >>> expr = a ( \"x\" ) + a ( \"y\" ) >>> expr . parse ( \"xy\" ) ('x', 'y') >>> expr = a ( \"x\" ) + a ( \"y\" ) + a ( \"z\" ) >>> expr . parse ( \"xyz\" ) ('x', 'y', 'z') >>> expr = a ( \"x\" ) + a ( \"y\" ) >>> expr . parse ( \"xz\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'z', expected: 'y' funcparserlib . parser . Parser . __neg__ ( self ) special Return a parser that parses the same tokens, but its parsing result is ignored by the sequential + combinator. Type: (Parser[A, B]) -> _IgnoredParser[A] You can use it for throwing away elements of concrete syntax (e.g. \",\" , \";\" ). Examples: >>> expr = - a ( \"x\" ) + a ( \"y\" ) >>> expr . parse ( \"xy\" ) 'y' >>> expr = a ( \"x\" ) + - a ( \"y\" ) >>> expr . parse ( \"xy\" ) 'x' >>> expr = a ( \"x\" ) + - a ( \"y\" ) + a ( \"z\" ) >>> expr . parse ( \"xyz\" ) ('x', 'z') >>> expr = - a ( \"x\" ) + a ( \"y\" ) + - a ( \"z\" ) >>> expr . parse ( \"xyz\" ) 'y' >>> expr = - a ( \"x\" ) + a ( \"y\" ) >>> expr . parse ( \"yz\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'y', expected: 'x' >>> expr = a ( \"x\" ) + - a ( \"y\" ) >>> expr . parse ( \"xz\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'z', expected: 'y' Note You should not pass the resulting parser to any combinators other than + . You should have at least one non-skipped value in your p1 + p2 + ... + pN . The parsed value of -p is an internal _Ignored object, not intended for actual use. funcparserlib . parser . Parser . __or__ ( self , other ) special Choice combination of parsers. It runs this parser and returns its result. If the parser fails, it runs the other parser. Examples: >>> expr = a ( \"x\" ) | a ( \"y\" ) >>> expr . parse ( \"x\" ) 'x' >>> expr . parse ( \"y\" ) 'y' >>> expr . parse ( \"z\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'z', expected: 'x' or 'y' funcparserlib . parser . Parser . __rshift__ ( self , f ) special Transform the parsing result by applying the specified function. Type: (Callable[[B], C]) -> Parser[A, C] You can use it for transforming the parsed value into another value before including it into the parse tree (the AST). Examples: >>> def make_canonical_name ( s ): ... return s . lower () >>> expr = ( a ( \"D\" ) | a ( \"d\" )) >> make_canonical_name >>> expr . parse ( \"D\" ) 'd' >>> expr . parse ( \"d\" ) 'd' funcparserlib . parser . maybe ( p ) Return a parser that returns None if the parser p fails. Examples: >>> expr = maybe ( a ( \"x\" )) >>> expr . parse ( \"x\" ) 'x' >>> expr . parse ( \"y\" ) is None True funcparserlib . parser . many ( p ) Return a parser that applies the parser p as many times as it succeeds at parsing the tokens. Return a parser that infinitely applies the parser p to the input sequence of tokens as long as it successfully parses them. The parsed value is a list of the sequentially parsed values. Examples: >>> expr = many ( a ( \"x\" )) >>> expr . parse ( \"x\" ) ['x'] >>> expr . parse ( \"xx\" ) ['x', 'x'] >>> expr . parse ( \"xxxy\" ) # noqa ['x', 'x', 'x'] >>> expr . parse ( \"y\" ) [] funcparserlib . parser . oneplus ( p ) Return a parser that applies the parser p one or more times. A similar parser combinator many(p) means apply p zero or more times, whereas oneplus(p) means apply p one or more times. Examples: >>> expr = oneplus ( a ( \"x\" )) >>> expr . parse ( \"x\" ) ['x'] >>> expr . parse ( \"xx\" ) ['x', 'x'] >>> expr . parse ( \"y\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'y', expected: 'x' funcparserlib . parser . skip ( p ) An alias for -p . See also the docs for Parser.__neg__() . Extra: Parser Monad As a functional programmer, you might be pleased to know, that parsers in funcparserlib form a monad with Parser.bind() as >>= and pure() as return . We could have expressed other parsing combinators in terms of bind() , but would be inefficient in Python: # noinspection PyUnresolvedReferences class Parser : def __add__ ( self , other ): return self . bind ( lambda x : other . bind ( lambda y : pure (( x , y )))) def __rshift__ ( self , other ): return self . bind ( lambda x : pure ( x )) funcparserlib . parser . Parser . bind ( self , f ) Bind the parser to a monadic function that returns a new parser. Type: (Callable[[B], Parser[A, C]]) -> Parser[A, C] Also known as >>= in Haskell. Note You can parse any context-free grammar without resorting to bind . Due to its poor performance please use it only when you really need it. funcparserlib . parser . pure ( x ) Wrap any object into a parser. Type: (A) -> Parser[A, A] A pure parser doesn't touch the tokens sequence, it just returns its pure x value. Also known as return in Haskell.","title":"Parser"},{"location":"api/parser/#funcparserlibparser-functional-parsing-combinators","text":"Functional parsing combinators. Parsing combinators define an internal domain-specific language (DSL) for describing the parsing rules of a grammar. The DSL allows you to start with a few primitive parsers, then combine your parsers to get more complex ones, and finally cover the whole grammar you want to parse. The structure of the language: Class Parser All the primitives and combinators of the language return Parser objects It defines the main Parser.parse(tokens) method Primitive parsers tok(type, value) , a(value) , some(pred) , forward_decl() , finished Parser combinators p1 + p2 , p1 | p2 , p >> f , -p , maybe(p) , many(p) , oneplus(p) , skip(p) Abstraction Use regular Python variables p = ... # Expression of type Parser to define new rules (non-terminals) of your grammar Every time you apply one of the combinators, you get a new Parser object. In other words, the set of Parser objects is closed under the means of combination. Note We took the parsing combinators language from the book Introduction to Functional Programming and translated it from ML into Python.","title":"funcparserlib.parser \u2014 Functional parsing combinators"},{"location":"api/parser/#funcparserlib.parser.Parser","text":"A parser object that can parse a sequence of tokens or can be combined with other parsers using + , | , >> , many() , and other parsing combinators. Type: Parser[A, B] The generic variables in the type are: A \u2014 the type of the tokens in the sequence to parse, B \u2014 the type of the parsed value. In order to define a parser for your grammar: You start with primitive parsers by calling a(value) , some(pred) , forward_decl() , finished You use parsing combinators p1 + p2 , p1 | p2 , p >> f , many(p) , and others to combine parsers into a more complex parser You can assign complex parsers to variables to define names that correspond to the rules of your grammar Note The constructor Parser.__init__() is considered internal and may be changed in future versions. Use primitive parsers and parsing combinators to construct new parsers.","title":"Parser"},{"location":"api/parser/#funcparserlib.parser.Parser.parse","text":"Parse the sequence of tokens and return the parsed value. Type: (Sequence[A]) -> B It takes a sequence of tokens of arbitrary type A and returns the parsed value of arbitrary type B . If the parser fails to parse the tokens, it raises NoParseError . Note Although Parser.parse() can parse sequences of any objects (including str which is a sequence of str chars), the recommended way is parsing sequences of Token objects. You should use a regexp-based tokenizer make_tokenizer() defined in funcparserlib.lexer to convert your text into a sequence of Token objects before parsing it. You will get more readable parsing error messages (as Token objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar.","title":"parse()"},{"location":"api/parser/#funcparserlib.parser.Parser.define","text":"Define the parser created earlier as a forward declaration. Type: (Parser[A, B]) -> None Use p = forward_decl() in combination with p.define(...) to define recursive parsers. See the examples in the docs for forward_decl() .","title":"define()"},{"location":"api/parser/#funcparserlib.parser.Parser.named","text":"Specify the name of the parser for easier debugging. Type: (str) -> Parser[A, B] This name is used in the debug-level parsing log. You can also get it via the Parser.name attribute. Examples: >>> expr = ( a ( \"x\" ) + a ( \"y\" )) . named ( \"expr\" ) >>> expr . name 'expr' >>> expr = a ( \"x\" ) + a ( \"y\" ) >>> expr . name \"('x', 'y')\" Note You can enable the parsing log this way: import logging logging . basicConfig ( level = logging . DEBUG ) import funcparserlib.parser funcparserlib . parser . debug = True The way to enable the parsing log may be changed in future versions.","title":"named()"},{"location":"api/parser/#primitive-parsers","text":"","title":"Primitive Parsers"},{"location":"api/parser/#funcparserlib.parser.tok","text":"Return a parser that parses a Token and returns the string value of the token. Type: (str, Optional[str]) -> Parser[Token, str] You can match any token of the specified type or you can match a specific token by its type and value . Examples: >>> expr = tok ( \"expr\" ) >>> expr . parse ([ Token ( \"expr\" , \"foo\" )]) 'foo' >>> expr . parse ([ Token ( \"expr\" , \"bar\" )]) 'bar' >>> expr . parse ([ Token ( \"op\" , \"=\" )]) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: '=', expected: expr >>> expr = tok ( \"op\" , \"=\" ) >>> expr . parse ([ Token ( \"op\" , \"=\" )]) '=' >>> expr . parse ([ Token ( \"op\" , \"+\" )]) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: '+', expected: '=' Note In order to convert your text to parse into a sequence of Token objects, use a regexp-based tokenizer make_tokenizer() defined in funcparserlib.lexer . You will get more readable parsing error messages (as Token objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar.","title":"tok()"},{"location":"api/parser/#funcparserlib.parser.a","text":"Return a parser that parses a token if it's equal to value . Type: (A) -> Parser[A, A] Examples: >>> expr = a ( \"x\" ) >>> expr . parse ( \"x\" ) 'x' >>> expr . parse ( \"y\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'y', expected: 'x' Note Although Parser.parse() can parse sequences of any objects (including str which is a sequence of str chars), the recommended way is parsing sequences of Token objects. You should use a regexp-based tokenizer make_tokenizer() defined in funcparserlib.lexer to convert your text into a sequence of Token objects before parsing it. You will get more readable parsing error messages (as Token objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar.","title":"a()"},{"location":"api/parser/#funcparserlib.parser.some","text":"Return a parser that parses a token if it satisfies the predicate pred . Type: (Callable[[A], bool]) -> Parser[A, A] Examples: >>> expr = some ( lambda s : s . isalpha ()) . named ( 'alpha' ) >>> expr . parse ( \"x\" ) 'x' >>> expr . parse ( \"y\" ) 'y' >>> expr . parse ( \"1\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: '1', expected: alpha Warning The some() combinator is quite slow and may be changed or removed in future versions. If you need a parser for a token by its type (e.g. any identifier) and maybe its value, use tok(type[, value]) instead. You should use make_tokenizer() from funcparserlib.lexer to tokenize your text first.","title":"some()"},{"location":"api/parser/#funcparserlib.parser.forward_decl","text":"Return an undefined parser that can be used as a forward declaration. Type: Parser[Any, Any] Use p = forward_decl() in combination with p.define(...) to define recursive parsers. Examples: >>> expr = forward_decl () >>> expr . define ( a ( \"x\" ) + maybe ( expr ) + a ( \"y\" )) >>> expr . parse ( \"xxyy\" ) # noqa ('x', ('x', None, 'y'), 'y') >>> expr . parse ( \"xxy\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected end of input, expected: 'y' Note If you care about static types, you should add a type hint for your forward declaration, so that your type checker can check types in p.define(...) later: p : Parser [ str , int ] = forward_decl () p . define ( a ( \"x\" )) # Type checker error p . define ( a ( \"1\" ) >> int ) # OK","title":"forward_decl()"},{"location":"api/parser/#finished","text":"A parser that throws an exception if there are any unparsed tokens left in the sequence. Type: Parser[Any, None] Examples: >>> from funcparserlib.parser import a , finished >>> expr = a ( \"x\" ) + finished >>> expr . parse ( \"x\" ) ('x', None) >>> expr = a ( \"x\" ) + finished >>> expr . parse ( \"xy\" ) Traceback (most recent call last): ... funcparserlib.parser.NoParseError : got unexpected token: 'y', expected: end of input","title":"finished"},{"location":"api/parser/#parser-combinators","text":"","title":"Parser Combinators"},{"location":"api/parser/#funcparserlib.parser.Parser.__add__","text":"Sequential combination of parsers. It runs this parser, then the other parser. The return value of the resulting parser is a tuple of each parsed value in the sum of parsers. We merge all parsing results of p1 + p2 + ... + pN into a single tuple. It means that the parsing result may be a 2-tuple, a 3-tuple, a 4-tuple, etc. of parsed values. You avoid this by transforming the parsed pair into a new value using the >> combinator. You can also skip some parsing results in the resulting parsers by using -p or skip(p) for some parsers in your sum of parsers. It means that the parsing result might be a single value, not a tuple of parsed values. See the docs for Parser.__neg__() for more examples. Overloaded types (lots of them to provide stricter checking for the quite dynamic return type of this method): (self: Parser[A, B], _IgnoredParser[A]) -> Parser[A, B] (self: Parser[A, B], Parser[A, C]) -> _TupleParser[A, Tuple[B, C]] (self: _TupleParser[A, B], _IgnoredParser[A]) -> _TupleParser[A, B] (self: _TupleParser[A, B], Parser[A, Any]) -> Parser[A, Any] (self: _IgnoredParser[A], _IgnoredParser[A]) -> _IgnoredParser[A] (self: _IgnoredParser[A], Parser[A, C]) -> Parser[A, C] Examples: >>> expr = a ( \"x\" ) + a ( \"y\" ) >>> expr . parse ( \"xy\" ) ('x', 'y') >>> expr = a ( \"x\" ) + a ( \"y\" ) + a ( \"z\" ) >>> expr . parse ( \"xyz\" ) ('x', 'y', 'z') >>> expr = a ( \"x\" ) + a ( \"y\" ) >>> expr . parse ( \"xz\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'z', expected: 'y'","title":"__add__()"},{"location":"api/parser/#funcparserlib.parser.Parser.__neg__","text":"Return a parser that parses the same tokens, but its parsing result is ignored by the sequential + combinator. Type: (Parser[A, B]) -> _IgnoredParser[A] You can use it for throwing away elements of concrete syntax (e.g. \",\" , \";\" ). Examples: >>> expr = - a ( \"x\" ) + a ( \"y\" ) >>> expr . parse ( \"xy\" ) 'y' >>> expr = a ( \"x\" ) + - a ( \"y\" ) >>> expr . parse ( \"xy\" ) 'x' >>> expr = a ( \"x\" ) + - a ( \"y\" ) + a ( \"z\" ) >>> expr . parse ( \"xyz\" ) ('x', 'z') >>> expr = - a ( \"x\" ) + a ( \"y\" ) + - a ( \"z\" ) >>> expr . parse ( \"xyz\" ) 'y' >>> expr = - a ( \"x\" ) + a ( \"y\" ) >>> expr . parse ( \"yz\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'y', expected: 'x' >>> expr = a ( \"x\" ) + - a ( \"y\" ) >>> expr . parse ( \"xz\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'z', expected: 'y' Note You should not pass the resulting parser to any combinators other than + . You should have at least one non-skipped value in your p1 + p2 + ... + pN . The parsed value of -p is an internal _Ignored object, not intended for actual use.","title":"__neg__()"},{"location":"api/parser/#funcparserlib.parser.Parser.__or__","text":"Choice combination of parsers. It runs this parser and returns its result. If the parser fails, it runs the other parser. Examples: >>> expr = a ( \"x\" ) | a ( \"y\" ) >>> expr . parse ( \"x\" ) 'x' >>> expr . parse ( \"y\" ) 'y' >>> expr . parse ( \"z\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'z', expected: 'x' or 'y'","title":"__or__()"},{"location":"api/parser/#funcparserlib.parser.Parser.__rshift__","text":"Transform the parsing result by applying the specified function. Type: (Callable[[B], C]) -> Parser[A, C] You can use it for transforming the parsed value into another value before including it into the parse tree (the AST). Examples: >>> def make_canonical_name ( s ): ... return s . lower () >>> expr = ( a ( \"D\" ) | a ( \"d\" )) >> make_canonical_name >>> expr . parse ( \"D\" ) 'd' >>> expr . parse ( \"d\" ) 'd'","title":"__rshift__()"},{"location":"api/parser/#funcparserlib.parser.maybe","text":"Return a parser that returns None if the parser p fails. Examples: >>> expr = maybe ( a ( \"x\" )) >>> expr . parse ( \"x\" ) 'x' >>> expr . parse ( \"y\" ) is None True","title":"maybe()"},{"location":"api/parser/#funcparserlib.parser.many","text":"Return a parser that applies the parser p as many times as it succeeds at parsing the tokens. Return a parser that infinitely applies the parser p to the input sequence of tokens as long as it successfully parses them. The parsed value is a list of the sequentially parsed values. Examples: >>> expr = many ( a ( \"x\" )) >>> expr . parse ( \"x\" ) ['x'] >>> expr . parse ( \"xx\" ) ['x', 'x'] >>> expr . parse ( \"xxxy\" ) # noqa ['x', 'x', 'x'] >>> expr . parse ( \"y\" ) []","title":"many()"},{"location":"api/parser/#funcparserlib.parser.oneplus","text":"Return a parser that applies the parser p one or more times. A similar parser combinator many(p) means apply p zero or more times, whereas oneplus(p) means apply p one or more times. Examples: >>> expr = oneplus ( a ( \"x\" )) >>> expr . parse ( \"x\" ) ['x'] >>> expr . parse ( \"xx\" ) ['x', 'x'] >>> expr . parse ( \"y\" ) Traceback (most recent call last): ... parser.NoParseError : got unexpected token: 'y', expected: 'x'","title":"oneplus()"},{"location":"api/parser/#funcparserlib.parser.skip","text":"An alias for -p . See also the docs for Parser.__neg__() .","title":"skip()"},{"location":"api/parser/#extra-parser-monad","text":"As a functional programmer, you might be pleased to know, that parsers in funcparserlib form a monad with Parser.bind() as >>= and pure() as return . We could have expressed other parsing combinators in terms of bind() , but would be inefficient in Python: # noinspection PyUnresolvedReferences class Parser : def __add__ ( self , other ): return self . bind ( lambda x : other . bind ( lambda y : pure (( x , y )))) def __rshift__ ( self , other ): return self . bind ( lambda x : pure ( x ))","title":"Extra: Parser Monad"},{"location":"api/parser/#funcparserlib.parser.Parser.bind","text":"Bind the parser to a monadic function that returns a new parser. Type: (Callable[[B], Parser[A, C]]) -> Parser[A, C] Also known as >>= in Haskell. Note You can parse any context-free grammar without resorting to bind . Due to its poor performance please use it only when you really need it.","title":"bind()"},{"location":"api/parser/#funcparserlib.parser.pure","text":"Wrap any object into a parser. Type: (A) -> Parser[A, A] A pure parser doesn't touch the tokens sequence, it just returns its pure x value. Also known as return in Haskell.","title":"pure()"},{"location":"api/util/","text":"funcparserlib.util \u2014 Various utilities funcparserlib . util . pretty_tree ( x , kids , show ) Return a pseudo-graphic tree representation of the object x similar to the tree command in Unix. Type: (T, Callable[[T], List[T]], Callable[[T], str]) -> str It applies the parameter show (which is a function of type (T) -> str ) to get a textual representation of the objects to show. It applies the parameter kids (which is a function of type (T) -> List[T] ) to list the children of the object to show. Examples: >>> print ( pretty_tree ( ... [ \"foo\" , [ \"bar\" , \"baz\" ], \"quux\" ], ... lambda obj : obj if isinstance ( obj , list ) else [], ... lambda obj : \"[]\" if isinstance ( obj , list ) else str ( obj ), ... )) [] |-- foo |-- [] | |-- bar | `-- baz `-- quux","title":"Utilities"},{"location":"api/util/#funcparserlibutil-various-utilities","text":"","title":"funcparserlib.util \u2014 Various utilities"},{"location":"api/util/#funcparserlib.util.pretty_tree","text":"Return a pseudo-graphic tree representation of the object x similar to the tree command in Unix. Type: (T, Callable[[T], List[T]], Callable[[T], str]) -> str It applies the parameter show (which is a function of type (T) -> str ) to get a textual representation of the objects to show. It applies the parameter kids (which is a function of type (T) -> List[T] ) to list the children of the object to show. Examples: >>> print ( pretty_tree ( ... [ \"foo\" , [ \"bar\" , \"baz\" ], \"quux\" ], ... lambda obj : obj if isinstance ( obj , list ) else [], ... lambda obj : \"[]\" if isinstance ( obj , list ) else str ( obj ), ... )) [] |-- foo |-- [] | |-- bar | `-- baz `-- quux","title":"pretty_tree()"},{"location":"getting-started/","text":"Getting Started Intro In this guide, we will write a parser for a numeric expression calculator with a syntax similar to Python expressions. Writing a calculator is a common example in articles related to parsers and parsing techniques, so it is a good starting point in learning funcparserlib . You will learn how to write a parser of numeric expressions using funcparserlib . Here are some expression strings we want to be able to parse: 0 1 + 2 + 3 -1 + 2 ** 32 3.1415926 * (2 + 7.18281828e-1) * 42 We will parse these strings into trees of objects like this one: BinaryExpr('*') |-- BinaryExpr('*') | |-- 3.1415926 | `-- BinaryExpr('+') | |-- 2 | `-- 0.718281828 `-- 42 Diving In Here is the complete source code of the expression parser we are going to write. You are not supposed to understand it now. Just look at its shape and try to get some feeling about its structure. By the end of this guide, you will fully understand this code and will be able to write parsers for your own needs. >>> from typing import List , Tuple , Union >>> from dataclasses import dataclass >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token >>> from funcparserlib.parser import tok , Parser , many , forward_decl , finished >>> @dataclass ... class BinaryExpr : ... op : str ... left : \"Expr\" ... right : \"Expr\" >>> Expr = Union [ BinaryExpr , int , float ] >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] >>> def parse ( tokens : List [ Token ]) -> Expr : ... int_num = tok ( \"int\" ) >> int ... float_num = tok ( \"float\" ) >> float ... number = int_num | float_num ... ... expr : Parser [ Token , Expr ] = forward_decl () ... parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) ... primary = number | parenthesized ... power = primary + many ( op ( \"**\" ) + primary ) >> to_expr ... term = power + many (( op ( \"*\" ) | op ( \"/\" )) + power ) >> to_expr ... sum = term + many (( op ( \"+\" ) | op ( \"-\" )) + term ) >> to_expr ... expr . define ( sum ) ... ... document = expr + - finished ... ... return document . parse ( tokens ) >>> def op ( name : str ) -> Parser [ Token , str ]: ... return tok ( \"op\" , name ) >>> def to_expr ( args : Tuple [ Expr , List [ Tuple [ str , Expr ]]]) -> Expr : ... first , rest = args ... result = first ... for op , expr in rest : ... result = BinaryExpr ( op , result , expr ) ... return result Note The code examples in this guide are actually executable. You can clone the funcparserlib repository from GitHub and run the examples from the document via doctest : python3 -m doctest -v docs/getting-started/*.md Test the expression parser: >>> parse ( tokenize ( \"0\" )) 0 >>> parse ( tokenize ( \"1 + 2 + 3\" )) BinaryExpr(op='+', left=BinaryExpr(op='+', left=1, right=2), right=3) >>> parse ( tokenize ( \"-1 + 2 ** 32\" )) BinaryExpr(op='+', left=-1, right=BinaryExpr(op='**', left=2, right=32)) >>> parse ( tokenize ( \"3.1415926 * (2 + 7.18281828e-1) * 42\" )) BinaryExpr(op='*', left=BinaryExpr(op='*', left=3.1415926, right=BinaryExpr(op='+', left=2, right=0.718281828)), right=42) Next Now let's start learning how to write a numeric expression parser using funcparserlib . In the next chapter you will learn about the first step in parsing: tokenizing the input. It means splitting your input string into a sequence of tokens that are easier to parse.","title":"Getting Started - Intro"},{"location":"getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"getting-started/#intro","text":"In this guide, we will write a parser for a numeric expression calculator with a syntax similar to Python expressions. Writing a calculator is a common example in articles related to parsers and parsing techniques, so it is a good starting point in learning funcparserlib . You will learn how to write a parser of numeric expressions using funcparserlib . Here are some expression strings we want to be able to parse: 0 1 + 2 + 3 -1 + 2 ** 32 3.1415926 * (2 + 7.18281828e-1) * 42 We will parse these strings into trees of objects like this one: BinaryExpr('*') |-- BinaryExpr('*') | |-- 3.1415926 | `-- BinaryExpr('+') | |-- 2 | `-- 0.718281828 `-- 42","title":"Intro"},{"location":"getting-started/#diving-in","text":"Here is the complete source code of the expression parser we are going to write. You are not supposed to understand it now. Just look at its shape and try to get some feeling about its structure. By the end of this guide, you will fully understand this code and will be able to write parsers for your own needs. >>> from typing import List , Tuple , Union >>> from dataclasses import dataclass >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token >>> from funcparserlib.parser import tok , Parser , many , forward_decl , finished >>> @dataclass ... class BinaryExpr : ... op : str ... left : \"Expr\" ... right : \"Expr\" >>> Expr = Union [ BinaryExpr , int , float ] >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] >>> def parse ( tokens : List [ Token ]) -> Expr : ... int_num = tok ( \"int\" ) >> int ... float_num = tok ( \"float\" ) >> float ... number = int_num | float_num ... ... expr : Parser [ Token , Expr ] = forward_decl () ... parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) ... primary = number | parenthesized ... power = primary + many ( op ( \"**\" ) + primary ) >> to_expr ... term = power + many (( op ( \"*\" ) | op ( \"/\" )) + power ) >> to_expr ... sum = term + many (( op ( \"+\" ) | op ( \"-\" )) + term ) >> to_expr ... expr . define ( sum ) ... ... document = expr + - finished ... ... return document . parse ( tokens ) >>> def op ( name : str ) -> Parser [ Token , str ]: ... return tok ( \"op\" , name ) >>> def to_expr ( args : Tuple [ Expr , List [ Tuple [ str , Expr ]]]) -> Expr : ... first , rest = args ... result = first ... for op , expr in rest : ... result = BinaryExpr ( op , result , expr ) ... return result Note The code examples in this guide are actually executable. You can clone the funcparserlib repository from GitHub and run the examples from the document via doctest : python3 -m doctest -v docs/getting-started/*.md Test the expression parser: >>> parse ( tokenize ( \"0\" )) 0 >>> parse ( tokenize ( \"1 + 2 + 3\" )) BinaryExpr(op='+', left=BinaryExpr(op='+', left=1, right=2), right=3) >>> parse ( tokenize ( \"-1 + 2 ** 32\" )) BinaryExpr(op='+', left=-1, right=BinaryExpr(op='**', left=2, right=32)) >>> parse ( tokenize ( \"3.1415926 * (2 + 7.18281828e-1) * 42\" )) BinaryExpr(op='*', left=BinaryExpr(op='*', left=3.1415926, right=BinaryExpr(op='+', left=2, right=0.718281828)), right=42)","title":"Diving In"},{"location":"getting-started/#next","text":"Now let's start learning how to write a numeric expression parser using funcparserlib . In the next chapter you will learn about the first step in parsing: tokenizing the input. It means splitting your input string into a sequence of tokens that are easier to parse.","title":"Next"},{"location":"getting-started/parse-tree/","text":"Preparing the Parse Tree So far we have defined the parser for our calculator expressions language: >>> from typing import List >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token >>> from funcparserlib.parser import tok , Parser , many , forward_decl , finished >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] >>> def op ( name : str ) -> Parser [ Token , str ]: ... return tok ( \"op\" , name ) >>> int_str = tok ( \"int\" ) >>> float_str = tok ( \"float\" ) >>> number = int_str | float_str >>> expr = forward_decl () >>> parenthesized = op ( \"(\" ) + expr + op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >>> expr . define ( power ) >>> document = expr + finished Here is how its parse results look so far: >>> document . parse ( tokenize ( \"2 ** (3 ** 4)\" )) ('2', [('**', ('(', ('3', [('**', '4')]), ')'))], None) p >> f : Transforming Parse Results Let's start improving our parse results by converting numbers from str to int or float . We will use the Parser.__rshift__ combinator for that. p >> f takes a parser p and a function f of a single argument and returns a new parser, that applies f to the parse result of p . An integer parser that returns int values: >>> int_num : Parser [ Token , int ] = tok ( \"int\" ) >> int Note We specify the type hint for the parser only for clarity here. We wanted to highlight that >> here changes the output type of the parser from str to int . You may omit type hints for parsers and rely on type inference features of your text editor and type checker to get code completion and linting warnings: >>> int_num = tok ( \"int\" ) >> int The only combinator which type is not inferrable is forward_decl() . You should specify its type explicitly to get your parser fully type checked. Try it: >>> int_num . parse ( tokenize ( \"42\" )) 42 Let's redefine our number parser so that it returns either int or float : >>> from typing import Union >>> float_num : Parser [ Token , float ] = tok ( \"float\" ) >> float >>> number : Parser [ Token , Union [ int , float ]] = int_num | float_num Test it: >>> number . parse ( tokenize ( \"42\" )) 42 >>> number . parse ( tokenize ( \"3.14\" )) 3.14 -p : Skipping Parse Results Let's recall our nested parenthesized numbers example: >>> p = forward_decl () >>> p . define ( number | ( op ( \"(\" ) + p + op ( \")\" ))) Test it: >>> p . parse ( tokenize ( \"((1))\" )) ('(', ('(', 1, ')'), ')') We have successfully parsed numbers in nested parentheses, but we don't want to see parentheses in the parsing results. Let's skip them using the Parser.__neg__ combinator. It allows you to skip any parts of a sequence of parsers concatenated via p1 + p2 + ... + pN by using a unary -p operator on the ones you want to skip: >>> p = forward_decl () >>> p . define ( number | ( - op ( \"(\" ) + p + - op ( \")\" ))) The result is cleaner now: >>> p . parse ( tokenize ( \"1\" )) 1 >>> p . parse ( tokenize ( \"(1)\" )) 1 >>> p . parse ( tokenize ( \"((1))\" )) 1 Let's re-define our grammar using the Parser.__neg__ combinator to get rid of extra parentheses in the parse results, as well as of extra None returned by finished : >>> expr = forward_decl () >>> parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >>> expr . define ( power ) >>> document = expr + - finished Test it: >>> document . parse ( tokenize ( \"2 ** (3 ** 4)\" )) (2, [('**', (3, [('**', 4)]))]) User-Defined Classes for the Parse Tree We have many types of binary operators in our grammar, but we've defined only the ** power operator so far. Let's define them for * , / , + , - as well: >>> expr = forward_decl () >>> parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >>> term = power + many (( op ( \"*\" ) | op ( \"/\" )) + power ) >>> sum = term + many (( op ( \"+\" ) | op ( \"-\" )) + term ) >>> expr . define ( sum ) >>> document = expr + - finished Here we've introduced a hierarchy of nested parsers: expr -> sum -> term -> power -> primary -> parenthesized -> expr -> ... to reflect the order of calculations set by our operator priorities: + < * < ** < () . Test it: >>> document . parse ( tokenize ( \"1 * (2 + 0) ** 3\" )) (1, [], [('*', (2, [], [], [('+', (0, [], []))], [('**', 3)]))], []) It's hard to understand the results without proper user-defined classes for our expression types. We actually have 3 expression types: Integer numbers Floating point numbers Binary expressions For integers and floats we will use Python int and float classes. For binary expressions we'll introduce the BinaryExpr class: >>> from dataclasses import dataclass >>> @dataclass ... class BinaryExpr : ... op : str ... left : \"Expr\" ... right : \"Expr\" Since we don't use a common base class for our expressions, we have to define Expr as a union of possible expression types: >>> Expr = Union[BinaryExpr, int, float] Now let's define a function to transform the parse results of our binary operators into BinaryExpr objects. Take a look at our parsers of various binary expressions. You can infer that each of them returns (expression, list of (operator, expression)) . We will transform these nested tuples and lists into a tree of nested expressions by defining a function to_expr(args) and applying >> to_expr to our expression parsers: >>> from typing import Tuple >>> def to_expr ( args : Tuple [ Expr , List [ Tuple [ str , Expr ]]]) -> Expr : ... first , rest = args ... result = first ... for op , expr in rest : ... result = BinaryExpr ( op , result , expr ) ... return result Let's re-define our grammar using this transformation: >>> expr : Parser [ Token , Expr ] = forward_decl () >>> parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >> to_expr >>> term = power + many (( op ( \"*\" ) | op ( \"/\" )) + power ) >> to_expr >>> sum = term + many (( op ( \"+\" ) | op ( \"-\" )) + term ) >> to_expr >>> expr . define ( sum ) >>> document = expr + - finished Test it: >>> document . parse ( tokenize ( \"3.1415926 * (2 + 7.18281828e-1) * 42\" )) BinaryExpr(op='*', left=BinaryExpr(op='*', left=3.1415926, right=BinaryExpr(op='+', left=2, right=0.718281828)), right=42) Let's pretty-print it using the pretty_tree(x, kids, show) function: >>> from funcparserlib.util import pretty_tree >>> def pretty_expr ( expr : Expr ) -> str : ... ... def kids ( expr : Expr ) -> List [ Expr ]: ... if isinstance ( expr , BinaryExpr ): ... return [ expr . left , expr . right ] ... else : ... return [] ... ... def show ( expr : Expr ) -> str : ... if isinstance ( expr , BinaryExpr ): ... return f \"BinaryExpr( { expr . op !r} )\" ... else : ... return repr ( expr ) ... ... return pretty_tree ( expr , kids , show ) Test it: >>> print ( pretty_expr ( document . parse ( tokenize ( \"3.1415926 * (2 + 7.18281828e-1) * 42\" )))) BinaryExpr('*') |-- BinaryExpr('*') | |-- 3.1415926 | `-- BinaryExpr('+') | |-- 2 | `-- 0.718281828 `-- 42 Finally, we have a proper parse tree that is easy to understand and work with! Next We've finished writing our numeric expressions parser. If you want to learn more, let's discuss a few tips and tricks about parsing in the next chapter .","title":"Preparing the Parse Tree"},{"location":"getting-started/parse-tree/#preparing-the-parse-tree","text":"So far we have defined the parser for our calculator expressions language: >>> from typing import List >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token >>> from funcparserlib.parser import tok , Parser , many , forward_decl , finished >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] >>> def op ( name : str ) -> Parser [ Token , str ]: ... return tok ( \"op\" , name ) >>> int_str = tok ( \"int\" ) >>> float_str = tok ( \"float\" ) >>> number = int_str | float_str >>> expr = forward_decl () >>> parenthesized = op ( \"(\" ) + expr + op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >>> expr . define ( power ) >>> document = expr + finished Here is how its parse results look so far: >>> document . parse ( tokenize ( \"2 ** (3 ** 4)\" )) ('2', [('**', ('(', ('3', [('**', '4')]), ')'))], None)","title":"Preparing the Parse Tree"},{"location":"getting-started/parse-tree/#p-f-transforming-parse-results","text":"Let's start improving our parse results by converting numbers from str to int or float . We will use the Parser.__rshift__ combinator for that. p >> f takes a parser p and a function f of a single argument and returns a new parser, that applies f to the parse result of p . An integer parser that returns int values: >>> int_num : Parser [ Token , int ] = tok ( \"int\" ) >> int Note We specify the type hint for the parser only for clarity here. We wanted to highlight that >> here changes the output type of the parser from str to int . You may omit type hints for parsers and rely on type inference features of your text editor and type checker to get code completion and linting warnings: >>> int_num = tok ( \"int\" ) >> int The only combinator which type is not inferrable is forward_decl() . You should specify its type explicitly to get your parser fully type checked. Try it: >>> int_num . parse ( tokenize ( \"42\" )) 42 Let's redefine our number parser so that it returns either int or float : >>> from typing import Union >>> float_num : Parser [ Token , float ] = tok ( \"float\" ) >> float >>> number : Parser [ Token , Union [ int , float ]] = int_num | float_num Test it: >>> number . parse ( tokenize ( \"42\" )) 42 >>> number . parse ( tokenize ( \"3.14\" )) 3.14","title":"p &gt;&gt; f: Transforming Parse Results"},{"location":"getting-started/parse-tree/#-p-skipping-parse-results","text":"Let's recall our nested parenthesized numbers example: >>> p = forward_decl () >>> p . define ( number | ( op ( \"(\" ) + p + op ( \")\" ))) Test it: >>> p . parse ( tokenize ( \"((1))\" )) ('(', ('(', 1, ')'), ')') We have successfully parsed numbers in nested parentheses, but we don't want to see parentheses in the parsing results. Let's skip them using the Parser.__neg__ combinator. It allows you to skip any parts of a sequence of parsers concatenated via p1 + p2 + ... + pN by using a unary -p operator on the ones you want to skip: >>> p = forward_decl () >>> p . define ( number | ( - op ( \"(\" ) + p + - op ( \")\" ))) The result is cleaner now: >>> p . parse ( tokenize ( \"1\" )) 1 >>> p . parse ( tokenize ( \"(1)\" )) 1 >>> p . parse ( tokenize ( \"((1))\" )) 1 Let's re-define our grammar using the Parser.__neg__ combinator to get rid of extra parentheses in the parse results, as well as of extra None returned by finished : >>> expr = forward_decl () >>> parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >>> expr . define ( power ) >>> document = expr + - finished Test it: >>> document . parse ( tokenize ( \"2 ** (3 ** 4)\" )) (2, [('**', (3, [('**', 4)]))])","title":"-p: Skipping Parse Results"},{"location":"getting-started/parse-tree/#user-defined-classes-for-the-parse-tree","text":"We have many types of binary operators in our grammar, but we've defined only the ** power operator so far. Let's define them for * , / , + , - as well: >>> expr = forward_decl () >>> parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >>> term = power + many (( op ( \"*\" ) | op ( \"/\" )) + power ) >>> sum = term + many (( op ( \"+\" ) | op ( \"-\" )) + term ) >>> expr . define ( sum ) >>> document = expr + - finished Here we've introduced a hierarchy of nested parsers: expr -> sum -> term -> power -> primary -> parenthesized -> expr -> ... to reflect the order of calculations set by our operator priorities: + < * < ** < () . Test it: >>> document . parse ( tokenize ( \"1 * (2 + 0) ** 3\" )) (1, [], [('*', (2, [], [], [('+', (0, [], []))], [('**', 3)]))], []) It's hard to understand the results without proper user-defined classes for our expression types. We actually have 3 expression types: Integer numbers Floating point numbers Binary expressions For integers and floats we will use Python int and float classes. For binary expressions we'll introduce the BinaryExpr class: >>> from dataclasses import dataclass >>> @dataclass ... class BinaryExpr : ... op : str ... left : \"Expr\" ... right : \"Expr\" Since we don't use a common base class for our expressions, we have to define Expr as a union of possible expression types: >>> Expr = Union[BinaryExpr, int, float] Now let's define a function to transform the parse results of our binary operators into BinaryExpr objects. Take a look at our parsers of various binary expressions. You can infer that each of them returns (expression, list of (operator, expression)) . We will transform these nested tuples and lists into a tree of nested expressions by defining a function to_expr(args) and applying >> to_expr to our expression parsers: >>> from typing import Tuple >>> def to_expr ( args : Tuple [ Expr , List [ Tuple [ str , Expr ]]]) -> Expr : ... first , rest = args ... result = first ... for op , expr in rest : ... result = BinaryExpr ( op , result , expr ) ... return result Let's re-define our grammar using this transformation: >>> expr : Parser [ Token , Expr ] = forward_decl () >>> parenthesized = - op ( \"(\" ) + expr + - op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >> to_expr >>> term = power + many (( op ( \"*\" ) | op ( \"/\" )) + power ) >> to_expr >>> sum = term + many (( op ( \"+\" ) | op ( \"-\" )) + term ) >> to_expr >>> expr . define ( sum ) >>> document = expr + - finished Test it: >>> document . parse ( tokenize ( \"3.1415926 * (2 + 7.18281828e-1) * 42\" )) BinaryExpr(op='*', left=BinaryExpr(op='*', left=3.1415926, right=BinaryExpr(op='+', left=2, right=0.718281828)), right=42) Let's pretty-print it using the pretty_tree(x, kids, show) function: >>> from funcparserlib.util import pretty_tree >>> def pretty_expr ( expr : Expr ) -> str : ... ... def kids ( expr : Expr ) -> List [ Expr ]: ... if isinstance ( expr , BinaryExpr ): ... return [ expr . left , expr . right ] ... else : ... return [] ... ... def show ( expr : Expr ) -> str : ... if isinstance ( expr , BinaryExpr ): ... return f \"BinaryExpr( { expr . op !r} )\" ... else : ... return repr ( expr ) ... ... return pretty_tree ( expr , kids , show ) Test it: >>> print ( pretty_expr ( document . parse ( tokenize ( \"3.1415926 * (2 + 7.18281828e-1) * 42\" )))) BinaryExpr('*') |-- BinaryExpr('*') | |-- 3.1415926 | `-- BinaryExpr('+') | |-- 2 | `-- 0.718281828 `-- 42 Finally, we have a proper parse tree that is easy to understand and work with!","title":"User-Defined Classes for the Parse Tree"},{"location":"getting-started/parse-tree/#next","text":"We've finished writing our numeric expressions parser. If you want to learn more, let's discuss a few tips and tricks about parsing in the next chapter .","title":"Next"},{"location":"getting-started/parsing/","text":"Parsing Tokens So far we have defined the tokenizer for our calculator expressions language: >>> from typing import List >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] It results a list of tokens which we want to parse according to our expressions grammar: >>> from pprint import pprint >>> pprint ( tokenize ( \"3.1415926 * (2 + 7.18281828e-1) * 42\" )) [Token('float', '3.1415926'), Token('op', '*'), Token('op', '('), Token('int', '2'), Token('op', '+'), Token('float', '7.18281828e-1'), Token('op', ')'), Token('op', '*'), Token('int', '42')] Parser Combinators A parser is an object that takes input tokens and transforms them into a parse result. For example, a primitive parser tok(type, value) parses a single token of a certain type and, optionally, with a certain value. Parsing a single token is not exciting at all. The interesting part comes when you start combining parsers via parser combinators to build bigger parsers of complex token sequences. Parsers from funcparserlib.parser have a nice layered structure that allows you to express the grammar rules of the langauge you want to parse: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 Primitive Parsers \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 tok(type, value) forward_decl() \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 a(token) some(pred) finished \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 Parser Combinators \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 Parser \u2502 p1 + p2 p1 | p2 p >> f -p \u2502 \u2502 objects \u2502 \u2502 \u2502 \u2502 many(p) oneplus(p) maybe(p) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 Means of Abstraction \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Python assignments: = \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Python functions: def \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 You get a new Parser object each time you apply a parser combinator to your parsers. Therefore, the set of all parsers it closed under the operations defined by parser combinators. Parsers are regular Python objects of type Parser . It means that you can write arbitrary Python code that builds parser objects: assign parsers to variables, pass parsers as call arguments, get them as the return values of calls, etc. Note The type Parser is actually parameterized as Parser[T, V] where: T is the type of input tokens V is the type of the parse result Your text editor or type checker will provide better code completion and error checking for your parsing code based on the types defined in funcparserlib and their type inference capabilities. tok() : Parsing a Single Token Let's recall the expressions we would like to be able to parse: 0 1 + 2 + 3 -1 + 2 ** 32 3.1415926 * (2 + 7.18281828e-1) * 42 It looks like our grammar should have expressions that consist of numbers or nested expressions. Let's start with just numbers. We'll use tok(type, value) to create a primitive parser of a single integer token. Let's import it: >>> from funcparserlib.parser import tok Here is our parser of a single integer token. The string \"int\" is the type of the integer token spec for our tokenizer: >>> int_str = tok ( \"int\" ) Let's try it in action. In order to invoke a parser, we should pass a sequence of tokens to its Parser.parse(tokens) method: >>> int_str . parse ( tokenize ( \"42\" )) '42' Note Our parser returns integer numbers as strings at the moment. We'll cover transforming parse results and creating a proper parse tree in the next chapter. If the first token in the input is not of type \"int\" , our parser raises an exception: >>> int_str . parse ( tokenize ( \"+\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '+', expected: int p1 | p2 : Parsing Alternatives We want to support floating point numbers as well. We already know how to do it: >>> float_str = tok ( \"float\" ) Let's define our number expression as either an integer or a float number. We can parse alternatives using the Parser.__or__ combinator: >>> number = int_str | float_str Test it: >>> number . parse ( tokenize ( \"42\" )) '42' >>> number . parse ( tokenize ( \"3.14\" )) '3.14' >>> number . parse ( tokenize ( \"*\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '*', expected: int or float p1 + p2 : Parsing a Sequence Since we can parse numbers now, let's proceeed with expressions. The first expression we will parse is the power operator: 2 ** 32 We need a new parser combinator to parse sequences of tokens. We can combine parsers sequentially using the Parser.__add__ combinator. Let's try it on sequences of numbers first: >>> p = number + number Test it: >>> p . parse ( tokenize ( \"1 2\" )) ('1', '2') The sequence combinator returns its results as a tuple of the parse results of its arguments. The size of the resulting tuple depends on the number of the parsers in the sequence. Let's try it for three numbers: >>> p = number + number + number Test it: >>> p . parse ( tokenize ( \"1 2 3\" )) ('1', '2', '3') Back to parsing the power operator of our calculator expressions language. We will need to parse several different operator tokens besides \"**\" in our grammar, so let's define a helper function: >>> from funcparserlib.parser import Parser >>> def op ( name : str ) -> Parser [ Token , str ]: ... return tok ( \"op\" , name ) Let's define the parser of the power operator expressions using our new op(name) helper: >>> power = number + op ( \"**\" ) + number Test it: >>> power . parse ( tokenize ( \"2 ** 32\" )) ('2', '**', '32') many() : Parsing Repeated Parts We want to allow sequences of power operators. Let's parse the first number, followed by zero or more pairs of the power operator and a number. We'll use the many(p) combinator for that. Let's import it: >>> from funcparserlib.parser import many Here is our parser of sequences of power operators: >>> power = number + many ( op ( \"**\" ) + number ) Test it: >>> power . parse ( tokenize ( \"2 ** 3 ** 4\" )) ('2', [('**', '3'), ('**', '4')]) The many(p) combinator applies its argument parser p to the input sequence of tokens many times until it fails, returning a list of the results. If p fails to parse any tokens, many(p) still succeeds and returns an empty list: >>> power . parse ( tokenize ( \"1 + 2\" )) ('1', []) forward_decl() : Parsing Recursive Parts We want to allow using parentheses to specify the order of calculations. Ideally, we would like to write a recursive assignment like this one, but the Python syntax doesn't allow it: >>> expr = power | number | ( op ( \"(\" ) + expr + op ( \")\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NameError : name 'expr' is not defined We will use the forward_decl() parser to solve the recursive assignment problem: We create a forward declaration We use the declaration in other parsers We define the value of the declaration Let's start with a simple example first. We'll create a parser numbers in properly nested parentheses: >>> from funcparserlib.parser import forward_decl >>> p = forward_decl () >>> p . define ( number | ( op ( \"(\" ) + p + op ( \")\" ))) Test it: >>> p . parse ( tokenize ( \"1\" )) '1' >>> p . parse ( tokenize ( \"(1)\" )) ('(', '1', ')') >>> p . parse ( tokenize ( \"((1))\" )) ('(', ('(', '1', ')'), ')') Back to our recursive expr problem. Let's re-write our grammar using forward_decl() for expressions: >>> expr = forward_decl () >>> parenthesized = op ( \"(\" ) + expr + op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >>> expr . define ( power ) Test it: >>> expr . parse ( tokenize ( \"2 ** 3 ** 4\" )) ('2', [('**', '3'), ('**', '4')]) >>> expr . parse ( tokenize ( \"2 ** (3 ** 4)\" )) ('2', [('**', ('(', ('3', [('**', '4')]), ')'))]) finished : Expecting No More Input Surprisingly, our expr parser tolerates incomplete expressions by ignoring the incomplete parts: >>> expr . parse ( tokenize ( \"2 ** (3 ** 4\" )) ('2', []) The problem is that its many(p) part parses the input while p succeeds, and it doesn't look any further than that. We can make a parser expect the end of the input via the finished parser. Let's define a parser for the whole input document: >>> from funcparserlib.parser import finished >>> document = expr + finished Note Usually you finish the topmost parser of your grammar with ... + finished to indicate that you expect no further input. Let's try it for our grammar: >>> document . parse ( tokenize ( \"2 ** (3 ** 4\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : got unexpected end of input, expected: ')' >>> document . parse ( tokenize ( \"2 ** (3 ** 4)\" )) ('2', [('**', ('(', ('3', [('**', '4')]), ')'))], None) Next We have created a parser for power operator expressions. Its parse results are correct, but they look hard to undersand and work with: Our integer and floating point numbers are strings, not int or float objects The results contain '(' and ')' strings even though we need parentheses only temporarily to set the operator priorities The results contain None , which is the parse result of finished , even though we don't need it The results are lists of tuples of strings, not user-defined classes that reflect the grammar of our calculator expressions language In the next chapter you will learn how to transform parse results and prepare a proper, cleaned up parse tree.","title":"Parsing Tokens"},{"location":"getting-started/parsing/#parsing-tokens","text":"So far we have defined the tokenizer for our calculator expressions language: >>> from typing import List >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] It results a list of tokens which we want to parse according to our expressions grammar: >>> from pprint import pprint >>> pprint ( tokenize ( \"3.1415926 * (2 + 7.18281828e-1) * 42\" )) [Token('float', '3.1415926'), Token('op', '*'), Token('op', '('), Token('int', '2'), Token('op', '+'), Token('float', '7.18281828e-1'), Token('op', ')'), Token('op', '*'), Token('int', '42')]","title":"Parsing Tokens"},{"location":"getting-started/parsing/#parser-combinators","text":"A parser is an object that takes input tokens and transforms them into a parse result. For example, a primitive parser tok(type, value) parses a single token of a certain type and, optionally, with a certain value. Parsing a single token is not exciting at all. The interesting part comes when you start combining parsers via parser combinators to build bigger parsers of complex token sequences. Parsers from funcparserlib.parser have a nice layered structure that allows you to express the grammar rules of the langauge you want to parse: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 Primitive Parsers \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 tok(type, value) forward_decl() \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 a(token) some(pred) finished \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 Parser Combinators \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 Parser \u2502 p1 + p2 p1 | p2 p >> f -p \u2502 \u2502 objects \u2502 \u2502 \u2502 \u2502 many(p) oneplus(p) maybe(p) \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 Means of Abstraction \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Python assignments: = \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 Python functions: def \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 You get a new Parser object each time you apply a parser combinator to your parsers. Therefore, the set of all parsers it closed under the operations defined by parser combinators. Parsers are regular Python objects of type Parser . It means that you can write arbitrary Python code that builds parser objects: assign parsers to variables, pass parsers as call arguments, get them as the return values of calls, etc. Note The type Parser is actually parameterized as Parser[T, V] where: T is the type of input tokens V is the type of the parse result Your text editor or type checker will provide better code completion and error checking for your parsing code based on the types defined in funcparserlib and their type inference capabilities.","title":"Parser Combinators"},{"location":"getting-started/parsing/#tok-parsing-a-single-token","text":"Let's recall the expressions we would like to be able to parse: 0 1 + 2 + 3 -1 + 2 ** 32 3.1415926 * (2 + 7.18281828e-1) * 42 It looks like our grammar should have expressions that consist of numbers or nested expressions. Let's start with just numbers. We'll use tok(type, value) to create a primitive parser of a single integer token. Let's import it: >>> from funcparserlib.parser import tok Here is our parser of a single integer token. The string \"int\" is the type of the integer token spec for our tokenizer: >>> int_str = tok ( \"int\" ) Let's try it in action. In order to invoke a parser, we should pass a sequence of tokens to its Parser.parse(tokens) method: >>> int_str . parse ( tokenize ( \"42\" )) '42' Note Our parser returns integer numbers as strings at the moment. We'll cover transforming parse results and creating a proper parse tree in the next chapter. If the first token in the input is not of type \"int\" , our parser raises an exception: >>> int_str . parse ( tokenize ( \"+\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '+', expected: int","title":"tok(): Parsing a Single Token"},{"location":"getting-started/parsing/#p1-p2-parsing-alternatives","text":"We want to support floating point numbers as well. We already know how to do it: >>> float_str = tok ( \"float\" ) Let's define our number expression as either an integer or a float number. We can parse alternatives using the Parser.__or__ combinator: >>> number = int_str | float_str Test it: >>> number . parse ( tokenize ( \"42\" )) '42' >>> number . parse ( tokenize ( \"3.14\" )) '3.14' >>> number . parse ( tokenize ( \"*\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '*', expected: int or float","title":"p1 | p2: Parsing Alternatives"},{"location":"getting-started/parsing/#p1-p2-parsing-a-sequence","text":"Since we can parse numbers now, let's proceeed with expressions. The first expression we will parse is the power operator: 2 ** 32 We need a new parser combinator to parse sequences of tokens. We can combine parsers sequentially using the Parser.__add__ combinator. Let's try it on sequences of numbers first: >>> p = number + number Test it: >>> p . parse ( tokenize ( \"1 2\" )) ('1', '2') The sequence combinator returns its results as a tuple of the parse results of its arguments. The size of the resulting tuple depends on the number of the parsers in the sequence. Let's try it for three numbers: >>> p = number + number + number Test it: >>> p . parse ( tokenize ( \"1 2 3\" )) ('1', '2', '3') Back to parsing the power operator of our calculator expressions language. We will need to parse several different operator tokens besides \"**\" in our grammar, so let's define a helper function: >>> from funcparserlib.parser import Parser >>> def op ( name : str ) -> Parser [ Token , str ]: ... return tok ( \"op\" , name ) Let's define the parser of the power operator expressions using our new op(name) helper: >>> power = number + op ( \"**\" ) + number Test it: >>> power . parse ( tokenize ( \"2 ** 32\" )) ('2', '**', '32')","title":"p1 + p2: Parsing a Sequence"},{"location":"getting-started/parsing/#many-parsing-repeated-parts","text":"We want to allow sequences of power operators. Let's parse the first number, followed by zero or more pairs of the power operator and a number. We'll use the many(p) combinator for that. Let's import it: >>> from funcparserlib.parser import many Here is our parser of sequences of power operators: >>> power = number + many ( op ( \"**\" ) + number ) Test it: >>> power . parse ( tokenize ( \"2 ** 3 ** 4\" )) ('2', [('**', '3'), ('**', '4')]) The many(p) combinator applies its argument parser p to the input sequence of tokens many times until it fails, returning a list of the results. If p fails to parse any tokens, many(p) still succeeds and returns an empty list: >>> power . parse ( tokenize ( \"1 + 2\" )) ('1', [])","title":"many(): Parsing Repeated Parts"},{"location":"getting-started/parsing/#forward_decl-parsing-recursive-parts","text":"We want to allow using parentheses to specify the order of calculations. Ideally, we would like to write a recursive assignment like this one, but the Python syntax doesn't allow it: >>> expr = power | number | ( op ( \"(\" ) + expr + op ( \")\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NameError : name 'expr' is not defined We will use the forward_decl() parser to solve the recursive assignment problem: We create a forward declaration We use the declaration in other parsers We define the value of the declaration Let's start with a simple example first. We'll create a parser numbers in properly nested parentheses: >>> from funcparserlib.parser import forward_decl >>> p = forward_decl () >>> p . define ( number | ( op ( \"(\" ) + p + op ( \")\" ))) Test it: >>> p . parse ( tokenize ( \"1\" )) '1' >>> p . parse ( tokenize ( \"(1)\" )) ('(', '1', ')') >>> p . parse ( tokenize ( \"((1))\" )) ('(', ('(', '1', ')'), ')') Back to our recursive expr problem. Let's re-write our grammar using forward_decl() for expressions: >>> expr = forward_decl () >>> parenthesized = op ( \"(\" ) + expr + op ( \")\" ) >>> primary = number | parenthesized >>> power = primary + many ( op ( \"**\" ) + primary ) >>> expr . define ( power ) Test it: >>> expr . parse ( tokenize ( \"2 ** 3 ** 4\" )) ('2', [('**', '3'), ('**', '4')]) >>> expr . parse ( tokenize ( \"2 ** (3 ** 4)\" )) ('2', [('**', ('(', ('3', [('**', '4')]), ')'))])","title":"forward_decl(): Parsing Recursive Parts"},{"location":"getting-started/parsing/#finished-expecting-no-more-input","text":"Surprisingly, our expr parser tolerates incomplete expressions by ignoring the incomplete parts: >>> expr . parse ( tokenize ( \"2 ** (3 ** 4\" )) ('2', []) The problem is that its many(p) part parses the input while p succeeds, and it doesn't look any further than that. We can make a parser expect the end of the input via the finished parser. Let's define a parser for the whole input document: >>> from funcparserlib.parser import finished >>> document = expr + finished Note Usually you finish the topmost parser of your grammar with ... + finished to indicate that you expect no further input. Let's try it for our grammar: >>> document . parse ( tokenize ( \"2 ** (3 ** 4\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : got unexpected end of input, expected: ')' >>> document . parse ( tokenize ( \"2 ** (3 ** 4)\" )) ('2', [('**', ('(', ('3', [('**', '4')]), ')'))], None)","title":"finished: Expecting No More Input"},{"location":"getting-started/parsing/#next","text":"We have created a parser for power operator expressions. Its parse results are correct, but they look hard to undersand and work with: Our integer and floating point numbers are strings, not int or float objects The results contain '(' and ')' strings even though we need parentheses only temporarily to set the operator priorities The results contain None , which is the parse result of finished , even though we don't need it The results are lists of tuples of strings, not user-defined classes that reflect the grammar of our calculator expressions language In the next chapter you will learn how to transform parse results and prepare a proper, cleaned up parse tree.","title":"Next"},{"location":"getting-started/tips-and-tricks/","text":"Tips and Tricks Let's use the tokenizer we have defined previously for our examples in this chapter: >>> from typing import List >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token >>> from funcparserlib.parser import tok , Parser , many , forward_decl , finished >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] >>> def op ( name : str ) -> Parser [ Token , str ]: ... return tok ( \"op\" , name ) Name Alternative Parsers for Better Error Messages Consider the following grammar: >>> number = ( tok ( \"int\" ) >> int ) | ( tok ( \"float\" ) >> float ) >>> paren = - op ( \"(\" ) + number + - op ( \")\" ) >>> mul = number + op ( \"*\" ) + number >>> expr = paren | mul When a parser fails to parse its input, it usually reports the token it expects: >>> paren . parse ( tokenize ( \"(1\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : got unexpected end of input, expected: ')' If there were several parsing alternatives, the parser will report an error after the longest successfully parsed sequence: >>> expr . parse ( tokenize ( \"1 + 2\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,3-1,3: got unexpected token: '+', expected: '*' If there were several parsing alternatives and all of them failed to parse the current token, then the parser will report its name as the expected input: >>> number . parse ( tokenize ( \"*\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '*', expected: int or float >>> expr . parse ( tokenize ( \"+\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '+', expected: int or float or (('(', int or float), ')') Parser names are auto-generated and may be quite long and hard to understand. For better error messages you may want to name your parsers explicitly via Parser.named(name) . The naming style is up to you. For example: >>> number = (( tok ( \"int\" ) >> int ) | ( tok ( \"float\" ) >> float )) . named ( \"number\" ) >>> paren = - op ( \"(\" ) + number + - op ( \")\" ) >>> mul = number + op ( \"*\" ) + number >>> expr = ( paren | mul ) . named ( \"number or '('\" ) Test it: >>> number . parse ( tokenize ( \"*\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '*', expected: number >>> expr . parse ( tokenize ( \"+\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '+', expected: number or '(' How to Handle Conflicting Alternatives If one of the parsing alternatives is a subpart of another one, then you should put the longest alternative first. Otherwise parsing the shorter one will make another one unreachable: >>> p = ( number + number ) | ( number + number + number ) >>> p . parse ( tokenize ( \"1 2 3\" )) (1, 2) Parse the longest alternative first: >>> p = ( number + number + number ) | ( number + number ) >>> p . parse ( tokenize ( \"1 2 3\" )) (1, 2, 3) >>> p . parse ( tokenize ( \"1 2\" )) (1, 2) Watch Out for Left Recursion There are certain kinds grammar rules you cannot use with funcparserlib . These are the rules that contain recursion in their leftmost parts. These rules lead to infinite recursion during parsing, that results in a RecursionError exception. For example, we want to define an expression expr either a multiplication operator mul or a number . We also want an expression to be a sequence of an expression expr , followed by an operator \"**\" , followed by another expression expr : >>> expr = forward_decl () >>> mul = expr + op ( \"*\" ) + expr >>> expr . define ( mul | number ) This looks reasonable at the first glance, but it contains left recursion. In order to parse the first token for expr , we need to parse the first token for mul , for that we need to parse the first token for expr , and so on. This left recursion in your grammar results in a stack overflow exception: >>> expr . parse ( tokenize ( \"1 * 2\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... RecursionError : maximum recursion depth exceeded You should think how to re-write your grammar to avoid left-recursive definitions. In our case of several multiplication operators we really want a number, followed by zero or more pairs of * and number: >>> expr = forward_decl () >>> mul = number + many ( op ( \"**\" ) + number ) >>> expr . define ( mul ) Test it: >>> expr . parse ( tokenize ( \"1 ** 2\" )) (1, [('**', 2)]) >>> expr . parse ( tokenize ( \"3\" )) (3, []) Remember that your parsers have to consume at least one token from the input before going into recursive defintions.","title":"Tips and Tricks"},{"location":"getting-started/tips-and-tricks/#tips-and-tricks","text":"Let's use the tokenizer we have defined previously for our examples in this chapter: >>> from typing import List >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token >>> from funcparserlib.parser import tok , Parser , many , forward_decl , finished >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] >>> def op ( name : str ) -> Parser [ Token , str ]: ... return tok ( \"op\" , name )","title":"Tips and Tricks"},{"location":"getting-started/tips-and-tricks/#name-alternative-parsers-for-better-error-messages","text":"Consider the following grammar: >>> number = ( tok ( \"int\" ) >> int ) | ( tok ( \"float\" ) >> float ) >>> paren = - op ( \"(\" ) + number + - op ( \")\" ) >>> mul = number + op ( \"*\" ) + number >>> expr = paren | mul When a parser fails to parse its input, it usually reports the token it expects: >>> paren . parse ( tokenize ( \"(1\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : got unexpected end of input, expected: ')' If there were several parsing alternatives, the parser will report an error after the longest successfully parsed sequence: >>> expr . parse ( tokenize ( \"1 + 2\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,3-1,3: got unexpected token: '+', expected: '*' If there were several parsing alternatives and all of them failed to parse the current token, then the parser will report its name as the expected input: >>> number . parse ( tokenize ( \"*\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '*', expected: int or float >>> expr . parse ( tokenize ( \"+\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '+', expected: int or float or (('(', int or float), ')') Parser names are auto-generated and may be quite long and hard to understand. For better error messages you may want to name your parsers explicitly via Parser.named(name) . The naming style is up to you. For example: >>> number = (( tok ( \"int\" ) >> int ) | ( tok ( \"float\" ) >> float )) . named ( \"number\" ) >>> paren = - op ( \"(\" ) + number + - op ( \")\" ) >>> mul = number + op ( \"*\" ) + number >>> expr = ( paren | mul ) . named ( \"number or '('\" ) Test it: >>> number . parse ( tokenize ( \"*\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '*', expected: number >>> expr . parse ( tokenize ( \"+\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... NoParseError : 1,1-1,1: got unexpected token: '+', expected: number or '('","title":"Name Alternative Parsers for Better Error Messages"},{"location":"getting-started/tips-and-tricks/#how-to-handle-conflicting-alternatives","text":"If one of the parsing alternatives is a subpart of another one, then you should put the longest alternative first. Otherwise parsing the shorter one will make another one unreachable: >>> p = ( number + number ) | ( number + number + number ) >>> p . parse ( tokenize ( \"1 2 3\" )) (1, 2) Parse the longest alternative first: >>> p = ( number + number + number ) | ( number + number ) >>> p . parse ( tokenize ( \"1 2 3\" )) (1, 2, 3) >>> p . parse ( tokenize ( \"1 2\" )) (1, 2)","title":"How to Handle Conflicting Alternatives"},{"location":"getting-started/tips-and-tricks/#watch-out-for-left-recursion","text":"There are certain kinds grammar rules you cannot use with funcparserlib . These are the rules that contain recursion in their leftmost parts. These rules lead to infinite recursion during parsing, that results in a RecursionError exception. For example, we want to define an expression expr either a multiplication operator mul or a number . We also want an expression to be a sequence of an expression expr , followed by an operator \"**\" , followed by another expression expr : >>> expr = forward_decl () >>> mul = expr + op ( \"*\" ) + expr >>> expr . define ( mul | number ) This looks reasonable at the first glance, but it contains left recursion. In order to parse the first token for expr , we need to parse the first token for mul , for that we need to parse the first token for expr , and so on. This left recursion in your grammar results in a stack overflow exception: >>> expr . parse ( tokenize ( \"1 * 2\" )) # doctest: +IGNORE_EXCEPTION_DETAIL Traceback (most recent call last): ... RecursionError : maximum recursion depth exceeded You should think how to re-write your grammar to avoid left-recursive definitions. In our case of several multiplication operators we really want a number, followed by zero or more pairs of * and number: >>> expr = forward_decl () >>> mul = number + many ( op ( \"**\" ) + number ) >>> expr . define ( mul ) Test it: >>> expr . parse ( tokenize ( \"1 ** 2\" )) (1, [('**', 2)]) >>> expr . parse ( tokenize ( \"3\" )) (3, []) Remember that your parsers have to consume at least one token from the input before going into recursive defintions.","title":"Watch Out for Left Recursion"},{"location":"getting-started/tokenizing/","text":"Tokenizing Input Parsing is usually split into two steps: Tokenizing the input string into a sequence of tokens Parsing the tokens into a parse tree \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 str \u2502 \u2502 List[Token] \u2502 \u2502 Expr \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba tokenize() \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba parse() \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Tokens are larger pieces of the input text such as words, punctuation marks, spaces, etc. It's easier to parse a list of tokens than a string, since you can skip auxiliary tokens (spaces, newlines, commments) during tokenizing and focus on the main ones. Tokens usually track their position in the text, which is helpful in parsing error messages. Tokenizing with make_tokenizer() One of the most common ways to define tokens and tokenizing rules is via regular expressions. funcparserlib comes with the module funcparserlib.lexer for creating regexp-based tokenizers. Note Parsers defined with funcparserlib can work with any tokens. You can plug your custom tokenizers and token types or even parse raw strings as lists of character tokens. In this guide we will use the recommended way of writing tokenizers: make_tokenizer() from the funcparserlib.lexer module. Let's identify token types in our numeric expressions language: Whitespace Spaces, tabs, newlines Integer numbers 0 , 256 , -42 , ... Floating point numbers 3.1415 , 27.1828e-01 , ... Operators ( , ) , * , + , / , - , ** We will define our token specs and pass them to make_tokenizer() to generate our tokenizer. We will also drop whitespace tokens from the result, since we don't need them. Some imports first: >>> from typing import List >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token The tokenizer itself: >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] Warning Be careful with ordering your token specs and your regexps so that larger tokens come first before their smaller subparts. In our token specs: Float tokens should come before int tokens ** should come before * Let's try our tokenizer: >>> tokenize ( \"42 + 1337\" ) [Token('int', '42'), Token('op', '+'), Token('int', '1337')] The str() form of the token shows its position in the input text, also available via t.start and t.end : >>> [ str ( t ) for t in tokenize ( \"42 + 1337\" )] [\"1,1-1,2: int '42'\", \"1,4-1,4: op '+'\", \"1,6-1,9: int '1337'\"] Next We have tokenized an numeric expression string into a list of tokens. In the next chapter you will learn how to parse these tokens by defining a grammar for our numeric expressions language.","title":"Tokenizing Input"},{"location":"getting-started/tokenizing/#tokenizing-input","text":"Parsing is usually split into two steps: Tokenizing the input string into a sequence of tokens Parsing the tokens into a parse tree \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 str \u2502 \u2502 List[Token] \u2502 \u2502 Expr \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba tokenize() \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba parse() \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Tokens are larger pieces of the input text such as words, punctuation marks, spaces, etc. It's easier to parse a list of tokens than a string, since you can skip auxiliary tokens (spaces, newlines, commments) during tokenizing and focus on the main ones. Tokens usually track their position in the text, which is helpful in parsing error messages.","title":"Tokenizing Input"},{"location":"getting-started/tokenizing/#tokenizing-with-make_tokenizer","text":"One of the most common ways to define tokens and tokenizing rules is via regular expressions. funcparserlib comes with the module funcparserlib.lexer for creating regexp-based tokenizers. Note Parsers defined with funcparserlib can work with any tokens. You can plug your custom tokenizers and token types or even parse raw strings as lists of character tokens. In this guide we will use the recommended way of writing tokenizers: make_tokenizer() from the funcparserlib.lexer module. Let's identify token types in our numeric expressions language: Whitespace Spaces, tabs, newlines Integer numbers 0 , 256 , -42 , ... Floating point numbers 3.1415 , 27.1828e-01 , ... Operators ( , ) , * , + , / , - , ** We will define our token specs and pass them to make_tokenizer() to generate our tokenizer. We will also drop whitespace tokens from the result, since we don't need them. Some imports first: >>> from typing import List >>> from funcparserlib.lexer import make_tokenizer , TokenSpec , Token The tokenizer itself: >>> def tokenize ( s : str ) -> List [ Token ]: ... specs = [ ... TokenSpec ( \"whitespace\" , r \"\\s+\" ), ... TokenSpec ( \"float\" , r \"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\" ), ... TokenSpec ( \"int\" , r \"[+\\-]?\\d+\" ), ... TokenSpec ( \"op\" , r \"(\\*\\*)|[+\\-*/()]\" ), ... ] ... tokenizer = make_tokenizer ( specs ) ... return [ t for t in tokenizer ( s ) if t . type != \"whitespace\" ] Warning Be careful with ordering your token specs and your regexps so that larger tokens come first before their smaller subparts. In our token specs: Float tokens should come before int tokens ** should come before * Let's try our tokenizer: >>> tokenize ( \"42 + 1337\" ) [Token('int', '42'), Token('op', '+'), Token('int', '1337')] The str() form of the token shows its position in the input text, also available via t.start and t.end : >>> [ str ( t ) for t in tokenize ( \"42 + 1337\" )] [\"1,1-1,2: int '42'\", \"1,4-1,4: op '+'\", \"1,6-1,9: int '1337'\"]","title":"Tokenizing with make_tokenizer()"},{"location":"getting-started/tokenizing/#next","text":"We have tokenized an numeric expression string into a list of tokens. In the next chapter you will learn how to parse these tokens by defining a grammar for our numeric expressions language.","title":"Next"}]}